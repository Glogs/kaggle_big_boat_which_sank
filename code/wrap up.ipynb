{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We know the following, from the competition descrition.\n",
    "\n",
    "**VARIABLE DESCRIPTIONS**:\n",
    "\n",
    "- survival: Survival\n",
    "    (0 = No; 1 = Yes)\n",
    "- pclass: Passenger Class\n",
    "    (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- name: Name\n",
    "- sex: Sex\n",
    "- age: Age\n",
    "- sibsp: Number of Siblings/Spouses Aboard\n",
    "- parch: Number of Parents/Children Aboard\n",
    "- ticket: Ticket Number\n",
    "- fare: Passenger Fare\n",
    "- cabin: Cabin\n",
    "- embarked: Port of Embarkation\n",
    "     (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "**SPECIAL NOTES**:\n",
    "\n",
    "- Pclass is a proxy for socio-economic status (SES) -  1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "- Age is in Years; Fractional if Age less than One (1)\n",
    " If the Age is Estimated, it is in the form xx.5\n",
    "- With respect to the family relation variables (i.e. `sibsp` and `parch`)\n",
    "some relations were ignored.  The following are the definitions used\n",
    "for `sibsp` and `parch`.\n",
    "\n",
    "  - Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "  - Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n",
    "  - Parent:   Mother or Father of Passenger Aboard Titanic\n",
    "  - Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "\n",
    "Other family relatives excluded from this study include cousins,\n",
    "nephews/nieces, aunts/uncles, and in-laws.  Some children travelled\n",
    "only with a nanny, therefore `parch=0` for them.  As well, some\n",
    "travelled with very close friends or neighbors in a village, however,\n",
    "the definitions do not support such relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We don't have many data, so will use cross validation instead of a separate Validation set. This will give us a score not useful as a generalization error, but will use it anyway for selection (as it should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying final scoring via a downloaded version of survivors\n",
    "\n",
    "Data comes from [http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets](), where some other data sets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is an all empty record at the end\n",
    "compiled_data = pd.read_csv('../input/titanic3.csv').dropna(how='all')\n",
    "\n",
    "# The quotes in the names are wrong for `test.csv` and `train.csv`.\n",
    "#  In general we don't care, so they are modified on the fly for the generation of the output test set\n",
    "test_for_merger = test.copy()\n",
    "test_for_merger['Name'] = test_for_merger['Name'].apply(lambda x: x.replace('\"',''))\n",
    "compiled_data['Name'] = compiled_data['name'].apply(lambda x: x.replace('\"',''))\n",
    "\n",
    "# We use both `Name` and `Ticket` to merge, because some passenger have duplicated names (we were getting wrong lengths before)\n",
    "y_test = test_for_merger.merge(compiled_data, left_on=['Name', 'Ticket'], \n",
    "                    right_on=['Name', 'ticket'], how='left').rename(columns={'survived': 'Survived'})['Survived'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generalization_error(prediction):\n",
    "    print(((y_test - prediction) == 0).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above data just to run the generalization error calculation, not for training, not for anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to easily generate the CSV submission file when it is time to do so, we define the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def csv_from_prediction(prediction, filename='submission.csv'):\n",
    "    submission = pd.DataFrame(data={'PassengerId': test['PassengerId'], 'Survived': prediction.astype(int)})\n",
    "\n",
    "    # This is what we do if we don't use 'index=False' below\n",
    "    #submission.set_index('PassengerId', drop=True, inplace=True)\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's get our bases covered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train = train['Survived']\n",
    "X_train = train.drop('Survived', axis=1)\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.6+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['Embarked'].value_counts()\n",
    "# We can see 'S' is the most common value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles are collected from both sets, and ages set from global means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "titles = X_train['Name'].append(X_test['Name']).apply(lambda x: re.match('.*,([^\\.]+)\\..*', x)[1].strip())\n",
    "title_equivalences = {'Don': 'Mr', 'Dona': 'Mrs', 'Mlle': 'Miss', 'Mme': 'Mrs', 'Jonkheer': 'Lady', 'the Countess': 'Lady'}\n",
    "\n",
    "X_train['Title'] = X_train['Name'].apply(lambda x: re.match('.*,([^\\.]+)\\..*', x)[1].strip())\n",
    "X_test['Title'] = X_test['Name'].apply(lambda x: re.match('.*,([^\\.]+)\\..*', x)[1].strip())\n",
    "for k,v in title_equivalences.items():\n",
    "    X_train.loc[X_train['Title'] == k, 'Title'] = v\n",
    "    X_test.loc[X_test['Title'] == k, 'Title'] = v\n",
    "    \n",
    "#title_mapping = {v: k for k, v in enumerate(title_mapping)}  # A Dict\n",
    "#title_mapping['Mme'] = title_mapping['Mrs']\n",
    "#title_mapping['Mlle'] = title_mapping['Miss']\n",
    "#title_mapping['the Countess'] = title_mapping['Lady']\n",
    "#title_mapping['Don'] = title_mapping['Mr']\n",
    "#title_mapping['Dona'] = title_mapping['Mrs']\n",
    "#inverse_title_mapping = {v: k for k, v in title_mapping.items()}  # The inverse Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticket_sizes = (X_train['Ticket'].append(X_test['Ticket'])).value_counts()\n",
    "        \n",
    "def data_munge(data):\n",
    "    useless_fields = ['PassengerId']\n",
    "    data.drop(useless_fields, axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    # This is the most common value\n",
    "    data['Embarked'].fillna('S', inplace=True)\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass']) #.drop(['Sex_male', 'Embarked_C', 'Pclass_3'], axis=1)\n",
    "    data['GroupSize'] = data['Ticket'].apply(lambda x: ticket_sizes[x])\n",
    "    data['NameLength'] = data['Name'].apply(lambda x: len(x))\n",
    "    data['FamilyOnBoard'] = data['SibSp'] + data['Parch']\n",
    "        \n",
    "    # Titles and ages were already compiled globally, but the `Title` was not dummified (and won't be)\n",
    "    #data = pd.get_dummies(data, columns=['Title'])\n",
    "\n",
    "    data['Kid'] = np.zeros(np.shape(data['Title']))\n",
    "    data.loc[data['Title'] == 'Master', 'Kid'] = 1\n",
    "    data.loc[data['Title'] == 'Miss', 'Kid'] = 1\n",
    "\n",
    "    data['MumWithKid'] = np.zeros(np.shape(data['GroupSize']))\n",
    "    data.loc[(data['Parch'] == 1) & (data['GroupSize'] == 3) & \n",
    "               (data['Sex_female'] == 1) & (data['Kid'] == 0), 'MumWithKid'] = 1\n",
    "    \n",
    "    data['CabinFirstLetter'] = data['Cabin'].apply(lambda x: x[0] if type(x)=='str' else '')\n",
    "    data = pd.get_dummies(data, columns=['CabinFirstLetter']).drop(['Cabin', 'CabinFirstLetter_'], axis=1)\n",
    "    \n",
    "    data['Fare'] = data['Fare'].fillna(train['Fare'].mean())\n",
    "    \n",
    "    for k in titles.unique():\n",
    "        mean_title_age = X_train['Age'].append(X_test['Age']).loc[X_train['Title'].append(X_test['Title'])==k].mean()\n",
    "        data.loc[(data['Title'] == k) & data['Age'].isnull(), 'Age'] = mean_title_age\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = data_munge(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simplest attempts (baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A random assignment should give a 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52% of accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.517396\n",
       "False    0.482604\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_prediction = np.random.randint(0,2,len(y_train))\n",
    "print(\"{:.2f}% of accuracy\".format(1- abs(y_train - random_prediction).sum()/len(y_train)))\n",
    "((y_train - random_prediction) == 0).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A majority assignment should be better because the results are unbalanced, and that would be the benchmark to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.616162\n",
       "1    0.383838\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62% of accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     0.616162\n",
       "False    0.383838\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_prediction = np.full(len(y_train),0)\n",
    "print(\"{:.2f}% of accuracy\".format(1- abs(y_train - majority_prediction).sum()/len(y_train)))\n",
    "((y_train - majority_prediction) == 0).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Process Test the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = data_munge(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Title',\n",
       "       'Sex_female', 'Embarked_Q', 'Embarked_S', 'Pclass_1', 'Pclass_2',\n",
       "       'GroupSize', 'NameLength', 'FamilyOnBoard', 'Kid', 'MumWithKid'], dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in np.setdiff1d(X_train.columns.values, X_test.columns.values):\n",
    "    X_test[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Proper learning going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features_ignored = ['Name', 'Title', 'Ticket', 'Parch', 'SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73737374  0.75420875  0.76094276]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7991021324354658"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cross_val_score(DecisionTreeClassifier(), X_train.drop(features_ignored, axis=1), y_train))\n",
    "cross_val_score(RandomForestClassifier(n_estimators=10), X_train.drop(features_ignored, axis=1), y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82379349046015715"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=100, min_samples_split=19, min_samples_leaf=2),\n",
    "                X_train.drop(features_ignored, axis=1), y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
       "            min_samples_split=19, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, min_samples_split=19, min_samples_leaf=2)\n",
    "model.fit(X_train.drop(features_ignored, axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.779904\n",
      "False    0.220096\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test.drop(features_ignored, axis=1))\n",
    "generalization_error(prediction)\n",
    "# I used to keep this result, but now it is quite bad (and it improved just by cleaning the notebook um??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.763158\n",
      "False    0.236842\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train.drop(features_ignored, axis=1), y_train)\n",
    "prediction = model.predict(X_test.drop(features_ignored, axis=1))\n",
    "generalization_error(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tree_dot = export_graphviz(model.estimators_[1], out_file=None, feature_names=X_test.drop(['Name', 'Ticket', 'Cabin'], axis=1).columns, filled=True)\n",
    "#graphviz.Source(tree_dot, format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 784 ms, sys: 130 ms, total: 915 ms\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = { 'n_estimators': np.arange(20,200,20),\n",
    "               'min_samples_leaf': np.arange(1, 5),\n",
    "               'min_samples_split' : np.arange(4, 18,2),\n",
    "               'max_depth': np.arange(4,6, 1)}\n",
    "\n",
    "param_grid = { 'n_estimators': [80],\n",
    "               'min_samples_leaf': [3],\n",
    "               'min_samples_split' : [6],\n",
    "               'max_depth': [5]}\n",
    "grid = GridSearchCV(RandomForestClassifier(warm_start=True, n_jobs=-1), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train.drop(features_ignored, axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
       "            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=80, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82603815937149272"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.845118\n",
       "False    0.154882\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = grid.predict(X_train.drop(features_ignored, axis=1))\n",
    "((y_train - prediction) == 0).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.76555\n",
      "False    0.23445\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prediction = grid.predict(X_test.drop(features_ignored, axis=1))\n",
    "generalization_error(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refit with the parameters identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features=5, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=6, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(warm_start=True, n_jobs=-1,\n",
    "                             n_estimators=50,\n",
    "                             max_depth=4, max_features=5 ,\n",
    "                             min_samples_leaf=1, min_samples_split=6)\n",
    "rfc.fit(X_train.drop(features_ignored, axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.839506\n",
      "False    0.160494\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prediction = rfc.predict(X_train.drop(features_ignored, axis=1))\n",
    "print(((y_train - prediction) == 0).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.767943\n",
      "False    0.232057\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#prediction = grid.predict(X_test.drop(features_ignored, axis=1))\n",
    "#csv_from_prediction(prediction, filename='submission_tree_3.csv')\n",
    "\n",
    "prediction = rfc.predict(X_test.drop(features_ignored, axis=1))\n",
    "csv_from_prediction(prediction, filename='submission_tree_4.csv')\n",
    "generalization_error(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame()\n",
    "for est in rfc.estimators_:\n",
    "    feature_importances = feature_importances.append(pd.Series(est.feature_importances_, \n",
    "                                        index= X_test.drop(features_ignored, axis=1).columns),\n",
    "                             ignore_index=True)\n",
    "feature_importances.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A visualization from a previous Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tree = export_graphviz(model.estimators_[11], out_file=None,\n",
    "                       feature_names=X_test.drop(features_ignored, axis=1).columns, filled=True)\n",
    "graphviz.Source(tree, format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame()\n",
    "for est in model.estimators_:\n",
    "    feature_importances = feature_importances.append(pd.Series(est.feature_importances_, \n",
    "                                        index= X_test.drop(features_ignored, axis=1).columns),\n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_importances.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
